{"cells":[{"cell_type":"code","execution_count":null,"id":"c891f9ed-77c9-4606-90a7-bc71ba39bfe4","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["key_vault_name = 'kv_to-be-replaced'"]},{"cell_type":"code","execution_count":null,"id":"2f50e9f6-9b8d-4720-8709-fee849fd6759","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["from trident_token_library_wrapper import PyTridentTokenLibrary as tl\n","\n","def get_secrets_from_kv(kv_name, secret_name):\n","\n","    access_token = mssparkutils.credentials.getToken(\"keyvault\")\n","    kv_endpoint = f'https://{kv_name}.vault.azure.net/'\n","    return(tl.get_secret_with_token(kv_endpoint,secret_name,access_token))\n","\n","openai_api_type = \"azure\"\n","openai_api_version  = get_secrets_from_kv(key_vault_name,\"AZURE-OPENAI-VERSION\")\n","openai_api_base = get_secrets_from_kv(key_vault_name,\"AZURE-OPENAI-ENDPOINT\")\n","openai_api_key = get_secrets_from_kv(key_vault_name,\"AZURE-OPENAI-KEY\")"]},{"cell_type":"code","execution_count":null,"id":"15bb7b04-b5ce-4f65-800b-20e3ff4b1ac9","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["from pyspark.sql import functions as F\n","from pyspark.sql.types import StructType, StructField, StringType, IntegerType, ArrayType, MapType, LongType, TimestampType\n","import os\n","\n","folder_path = 'Files/data/conversation_input/'\n","\n","# Define the schema for the nested Messages in the Conversation\n","message_schema = StructType([\n","    StructField(\"Id\", StringType(), True),\n","    StructField(\"ReferenceId\", StringType(), True),\n","    StructField(\"EventType\", StringType(), True),\n","    StructField(\"EventTime\", StringType(), True),\n","    StructField(\"ConversationId\", StringType(), True),\n","    StructField(\"Value\", StringType(), True),\n","    StructField(\"UserId\", StringType(), True),\n","    StructField(\"CustomProperties\", MapType(StringType(), StringType()), True)\n","])\n","\n","# Define the schema for the Conversation\n","conversation_schema = StructType([\n","    StructField(\"ConversationId\", StringType(), True),\n","    StructField(\"Messages\", ArrayType(message_schema), True),\n","    StructField(\"StartTime\", TimestampType(), True),\n","    StructField(\"EndTime\", TimestampType(), True),\n","    StructField(\"Merged_content\", StringType(), True),\n","    StructField(\"Merged_content_user\", StringType(), True),\n","    StructField(\"Merged_content_agent\", StringType(), True),\n","    StructField(\"Full_conversation\", StringType(), True),\n","    StructField(\"Duration\", LongType(), True)  # New field for duration\n","])\n","\n","# Define the complete schema for the JSON document\n","schema = StructType([\n","    StructField(\"AgentName\", StringType(), True),\n","    StructField(\"AgentId\", StringType(), True),\n","    StructField(\"Team\", StringType(), True),\n","    StructField(\"ResolutionStatus\", StringType(), True),\n","    StructField(\"CallReason\", StringType(), True),\n","    StructField(\"CallerID\", StringType(), True),\n","    StructField(\"Conversation\", conversation_schema, True)\n","])\n","\n","df = None\n","df = spark.read.option(\"multiLine\", True).schema(schema).option(\"mode\", \"FAILFAST\").json(folder_path)\n","\n","#use the legacy time parser policy\n","spark.sql(\"set spark.sql.legacy.timeParserPolicy=LEGACY\")\n","\n","# Update Duration field with the duration from StartTime to Endtime in milliseconds\n","df = df.withColumn(\"Conversation\", df[\"Conversation\"].withField(\"Duration\", \n","                                                (F.unix_timestamp(df[\"Conversation\"][\"EndTime\"], 'yyyy-MM-dd\\'T\\'HH:mm:ss') - \n","                                                 F.unix_timestamp(df[\"Conversation\"][\"StartTime\"], 'yyyy-MM-dd\\'T\\'HH:mm:ss')) / 60))\n","\n","\n","# Create ConversationDate field based on StartTime and set to the beginning of the day\n","df = df.withColumn(\"Conversation\", df[\"Conversation\"].withField(\"ConversationDate\", \n","                                                F.date_trunc('day', df[\"Conversation\"][\"StartTime\"])))\n"]},{"cell_type":"code","execution_count":null,"id":"5a98c39c-b601-4104-afcf-d1506bcd3248","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["# Select specific columns, including nested ones\n","selected_df = df.select(\n","    \"AgentName\",\n","    \"AgentId\",\n","    \"Team\",\n","    \"ResolutionStatus\",\n","    \"CallReason\",\n","    \"CallerID\",\n","    \"Conversation.ConversationId\",\n","    \"Conversation.StartTime\",\n","    \"Conversation.EndTime\",\n","    \"Conversation.ConversationDate\",\n","    \"Conversation.Merged_content\",\n","    \"Conversation.Merged_content_user\",\n","    \"Conversation.Merged_content_agent\",\n","    \"Conversation.Full_conversation\",\n","    \"Conversation.Duration\"\n",")"]},{"cell_type":"code","execution_count":null,"id":"c18b80d9-ae79-4a4c-904d-c73315580106","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["# display(selected_df)"]},{"cell_type":"code","execution_count":null,"id":"d9b44b91-e279-405b-9380-36e9a0f38444","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["import os\n","import openai\n","import json\n","import time\n","import ast\n","import traceback\n","\n","# Function to get details from a conversation\n","def get_details(input_text):\n","    time.sleep(4)\n","\n","    openai.api_type = openai_api_type\n","    openai.api_version = openai_api_version\n","    openai.api_base = openai_api_base\n","    openai.api_key =  openai_api_key\n","\n","    # Construct the prompt for the OpenAI API\n","\n","    # Reference: For further details and guidance on how to effectively write metaprompt or system prompts, please refer to https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/system-message . Last Updated: 05/28/2024\n","    # prompt = '''You are a JSON formatter for extracting information out of a single chat conversation. \n","    #         Summarize the conversation in 20 words, key: summary .\n","    #         Is the customer satisfied with the agent interaction (Yes or No), key: satisfied . \n","    #         Identify the sentiment of the customer as (Positive, Negative or Neutral),key : avgSentiment . \n","    #         Normalize the conversation text by converting it to lowercase and trimming whitespace. Identify the single primary complaint of the conversation in 3 words or less. The complaint must always start with a noun and be a noun phrase. Key: Complaint.\n","    #         Identify the single primary compliment of the conversation in 6 words or less,key: Compliment . \n","    #         Identify the top 10 key phrases as comma separated string excluding people names , key: keyPhrases .\n","    #         Identify the main topic, key: topic .\n","    #         Identify the language of the text using ISO 639 two letter language identifier, key: lang .\n","    #         Answer in JSON machine-readable format, using the keys from above. \n","    #         Pretty print the JSON and make sure that it is properly closed at the end and do not generate any other content.'''\n","        \n","\n","    prompt = '''You are a JSON formatter for extracting information out of a single chat conversation. \n","        Summarize the conversation in 20 words, key: summary .\n","        Is the customer satisfied with the agent interaction (Satisfied or Dissatisfied), key: satisfied . \n","        Identify the sentiment of the customer as (Positive, Negative or Neutral),key : avgSentiment . \n","        Identify the origin city of travel,key: OriginCity . \n","        Identify the destination city of travel,key : DestinationCity . \n","        Normalize the conversation text by converting it to lowercase and trimming whitespace. Identify the single primary complaint of the conversation in 3 words or less. The complaint must always start with a noun and be a noun phrase (e.g., flight delay, room dirty, etc.). Key: Complaint.\n","        Identify the single primary compliment of the conversation in 6 words or less,key: Compliment . \n","        Identify the name of hotel that was mentioned,key: Hotel . \n","        Identify the name of airline if mentioned,key: Airline . \n","        Identify the name of the agent,key: AgentName .\n","        Identify the top 10 key phrases as comma seperated string excluding people names , key: keyPhrases .\n","        Identify the main topic, key: topic .\n","        Identify the language of the text using ISO 639 two letter language identifier, key: lang .\n","        Answer in JSON machine-readable format, using the keys from above. \n","        Pretty print the JSON and make sure that it is properly closed at the end and do not generate any other content.'''\n","\n","    # Add to prompt if desired:\n","    # Identify input_text translated to english, return the same text if already in english, key: translated_text .\n","\n","    # Set maximum number of retries\n","    max_retries = 4\n","    attempts = 0\n","    # print(\"attempts: \", attempts, \"max retries: \", max_retries)\n","\n","    # Loop until maximum retries are reached\n","    while attempts < max_retries:\n","        try:\n","            #print(input_text)\n","            response = openai.ChatCompletion.create(\n","            engine= \"gpt-4\",\n","            messages=[{\"role\": \"system\", \"content\": prompt},{\"role\": \"user\", \"content\": input_text}],\n","            response_format={\"type\": \"json_object\"})\n","\n","            # response = openai.ChatCompletion.create(\n","            # engine= \"gpt-35-turbo-16k\",\n","            # messages=[{\"role\": \"system\", \"content\": prompt},{\"role\": \"user\", \"content\": input_text}])\n","\n","           # Parse the response from the API\n","            result = ast.literal_eval(response['choices'][0]['message']['content'])\n","            # If 'summary' is found in the result, print and return the result\n","            if 'summary' in result and result['summary'] is not None and result['summary'].strip() != '':\n","                return result\n","            else:\n","                # If 'summary' is not found, increment attempts and try again\n","                attempts += 1\n","                print(f\"Attempt {attempts} failed. 'summary' not found in result. Trying again.\")\n","                time.sleep(40)\n","        except Exception as e:\n","            # If an error occurs, increment attempts and try again\n","            print(f\"Attempt {attempts} failed with error: {e}. Trying again. Full exception: {traceback.format_exc()}\")\n","            attempts += 1\n","            time.sleep(40)\n","\n","    print(\"Maximum number of retries reached. Exiting.\")\n","    return {\n","        'summary': '',\n","        'satisfied': '',\n","        'avgSentiment': '',\n","        'OriginCity': '',\n","        'DestinationCity': '',\n","        'Complaint': '',\n","        'Compliment': \"\",\n","        'Hotel': '',\n","        'Airline': '',\n","        'AgentName': '',\n","        'keyPhrases': '',\n","        'topic': '',\n","        'lang': ''\n","    }\n","    #,\n","    #     'translated_text': ''\n","    # }"]},{"cell_type":"code","execution_count":null,"id":"026b2c17-7263-43f1-ba54-1cba6edd2588","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["selected_df_pandas = selected_df.toPandas()"]},{"cell_type":"code","execution_count":null,"id":"3a52e75d-d4a7-43f8-b7c9-998d6f49aeb8","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["# display(selected_df_pandas)"]},{"cell_type":"code","execution_count":null,"id":"8f040473-363c-451f-95d3-2b4a342af83e","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["from pyspark.sql.types import *\n","\n","# Define the schema\n","schema = StructType([\n","    StructField('ConversationId', StringType(), True),\n","    StructField('ConversationDate', TimestampType(), True),\n","    StructField('EndTime', TimestampType(), True),\n","    StructField('StartTime', TimestampType(), True),\n","    StructField('Duration', DoubleType(), True),\n","    StructField('AgentId', StringType(), True),\n","    StructField('AgentName', StringType(), True),\n","    StructField('Team', StringType(), True),\n","    StructField('ResolutionStatus', StringType(), True),\n","    StructField('CallReason', StringType(), True),\n","    StructField('CallerID', StringType(), True),\n","    StructField('Merged_content', StringType(), True),\n","    StructField('Merged_content_agent', StringType(), True),\n","    StructField('Merged_content_user', StringType(), True),\n","    StructField('summary', StringType(), True),\n","    StructField('satisfied', StringType(), True),\n","    StructField('avgSentiment', StringType(), True),\n","    StructField('OriginCity', StringType(), True),\n","    StructField('DestinationCity', StringType(), True),\n","    StructField('Complaint', StringType(), True),\n","    StructField('Compliment', StringType(), True),\n","    StructField('Hotel', StringType(), True),\n","    StructField('Airline', StringType(), True),\n","    StructField('keyPhrases', StringType(), True),\n","    StructField('topic', StringType(), True),\n","    StructField('lang', StringType(), True)\n","])"]},{"cell_type":"code","execution_count":null,"id":"e50bd0a1-8bbc-4b5a-ba32-a123f81f3950","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["# This script iterates over a pandas DataFrame containing the conversations, enriches each row with additional details, \n","# converts timestamps, and displays the results as a Spark DataFrame.\n","\n","import pandas as pd\n","\n","# Initialize an empty list to store the results\n","res_list = []\n","\n","# Iterate over each row in the selected pandas DataFrame\n","for i, row in selected_df_pandas.iterrows():\n","    # print(f\"processing row {i}, ConversationID: {row.ConversationId}\")\n","    # Convert the row to a dictionary and merge it with the details obtained from the 'Merged_content' column\n","    result = row.to_dict() | get_details(row.Merged_content)\n","    # Convert pandas timestamp objects to Python datetime objects\n","    for key in ['ConversationDate', 'EndTime', 'StartTime']:\n","        if key in result and isinstance(result[key], pd.Timestamp):\n","            result[key] = result[key].to_pydatetime()\n","    # Append the result to the list\n","    res_list.append(result)\n","\n","# Create a Spark DataFrame from the list of results\n","df_processed = spark.createDataFrame(res_list, schema=schema)\n","\n","# Display the processed DataFrame\n","# display(df_processed)"]},{"cell_type":"code","execution_count":null,"id":"8892a884-ce0f-4966-8737-d6dd9604d6ff","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["# Select the columns in desired order\n","df_processed = df_processed.select([\"ConversationId\", \"ConversationDate\", \"EndTime\",\"StartTime\",\"Duration\",\"AgentId\",\"AgentName\",\"Team\",\"ResolutionStatus\",\"CallReason\",\"CallerID\", \"Merged_content\", \"Merged_content_agent\",\"Merged_content_user\", \\\n","                          \"summary\", \\\n","                          \"satisfied\", \\\n","                          \"avgSentiment\", \\\n","                          \"OriginCity\", \\\n","                          \"DestinationCity\", \\\n","                          \"Complaint\", \\\n","                          \"Compliment\", \\\n","                          \"Hotel\", \\\n","                          \"Airline\", \\\n","                          \"keyPhrases\", \\\n","                          \"topic\", \\\n","                          \"lang\"])\n","\n","# Display the DataFrame\n","# display(df_processed)\n"]},{"cell_type":"code","execution_count":null,"id":"5a0a7600-08c5-42a0-8aaa-a5128f279acc","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["#This code can be used for debugging\n","\n","# for i, row in selected_df_pandas.iterrows():\n","#     print(\"\")\n","#     print(f\"row {i}\")\n","#     print(f\"ConversationID: {row.ConversationId}\")\n","#     print(get_details(row.Merged_content))\n","#     # break"]},{"cell_type":"code","execution_count":null,"id":"4a7e32a2-29a4-4a49-9b7c-6673bb07eb10","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["# Drop the table if it exists\n","spark.sql('drop table if exists ckm_conv_processed_raw')"]},{"cell_type":"code","execution_count":null,"id":"f1436380-04e7-4d2d-907a-89bf760ef8d2","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["# Save processed records to ckm_conv_processed_raw table\n","df_processed.write.format('delta').mode('append').option(\"overwriteSchema\", \"true\").saveAsTable('ckm_conv_processed_raw')"]},{"cell_type":"code","execution_count":null,"id":"51fb894f-47a1-4439-adb4-90991d02abd7","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["# Move input files to processed directory\n","\n","import os\n","import shutil\n","\n","# Directory paths\n","input_dir = '/lakehouse/default/Files/data/conversation_input/'\n","processed_dir = '/lakehouse/default/Files/data/conversation_processed/'\n","\n","# Get a list of all .json files in the input directory\n","json_files = [f for f in os.listdir(input_dir) if f.endswith('.json')]\n","\n","# Move each .json file to the processed directory\n","for file_name in json_files:\n","    shutil.move(os.path.join(input_dir, file_name), os.path.join(processed_dir, file_name))"]},{"cell_type":"code","execution_count":null,"id":"4ea50892-6924-46d9-ab3a-7abf420bcc06","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["# display(df_processed)"]},{"cell_type":"code","execution_count":null,"id":"be1facf0-7ff9-4e4e-beaf-c9e796cab490","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["# df = spark.sql(\"SELECT * FROM ckm_conv_processed_raw LIMIT 1\")\n","# display(df)"]},{"cell_type":"code","execution_count":null,"id":"0fcedb3e-d5fc-4d5f-b531-012e8d32f4cf","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["# df = spark.sql(\"SELECT ConversationId,AgentId,CallerID,avgSentiment,lang,summary  FROM ckm_conv_processed_raw LIMIT 1000\")\n","# display(df)"]}],"metadata":{"dependencies":{"lakehouse":{"default_lakehouse":"e6ad9dad-e3da-4da5-bca6-6572c466b69a","default_lakehouse_name":"ckm_lakehouse","default_lakehouse_workspace_id":"0d98d480-171b-4b4d-a8e7-80fbd031d1a6"}},"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"display_name":"Synapse PySpark","language":"Python","name":"synapse_pyspark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"spark_compute":{"compute_id":"/trident/default"},"synapse_widget":{"state":{},"version":"0.1"},"widgets":{}},"nbformat":4,"nbformat_minor":5}
